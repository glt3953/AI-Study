欢迎来到Onboard,真实的一线经验,走新的投资思考。我是Monica。我是高宁。我们一起聊聊软件如何改变世界。大家好,欢迎来到Onboard,我是Monica。自从OpenAI发布的ChatGPT掀起了席卷世界的AI热潮,不到三个月就积累了超过一亿的越活用户,超过1300万的日活用户。真的是展现了AI让人惊叹的能力,也让很多人直呼这就是下一个互联网的未来。有不少观众都说希望我们再做一期AI的讨论,于是这次硬核讨论就来了。这次我们请来了Google Brain的研究员雪芝,她是Google大语言模型PALM Pathways Language Model的作者之一。要知道,这个模型的参数量是GPT-3的三倍还多。另外还有两位AI产品大牛,一位来自著名的Stable Diffusion背后的商业公司Certainty AI,另一位来自某硅谷科技大厂,也曾在吴恩达教授的Landing AI中担任产品负责人。此外,Monica还邀请到一位一直关注AI的投资人朋友Bill当做我的特邀共同主持嘉宾。我们主要讨论几个话题,一方面从研究的视角,最前沿的研究者在关注什么?现在的技术的天花板和未来大的变量可能会在哪里?从产品和商业的角度,什么是一个好的AI产品?整个生态可能随着技术有怎样的演变?更重要的,我们又能从上一波AI的创业热潮中学到什么?最后,Monica和Bill还会从投资人的视角做一个回顾、总结和畅想。这里还有一个小的update,在本集发布的时候,Google也对爆发式增长的Chat GPT做出了回应。正在测试一个基于Lambda模型的聊天机器人ApprenticeBot。证实发布后会有怎样的惊喜?我们都拭目以待。AI无疑是未来几年最令人兴奋的变量之一。Monica也希望未来能邀请到更多一线从业者从不同角度讨论这个话题。不论是想要做创业、研究、产品还是投资的同学,希望这些对话对于大家了解这些技术演进、商业的可能,甚至未来对于我们每个人、每个社会意味着什么都能引发一些思考,提供一些启发。这次的讨论有些技术硬核,需要各位对生成式AI大模型都有一些基础了解。讨论中涉及到的论文和重要概念也会总结在本集的简介中,供大家复习参考。几位嘉宾在北美工作生活多年,夹杂英文在所难免,也请大家体谅了。欢迎来到未来,大家enjoy!可以大家先做一个简单的自我介绍,你们自己过去的一些经验,一个fun fact,就是你最喜欢的一个生成式AI的项目。大家好,我的名字叫王雪志,我现在是在Google Brain做research scientist。我在Google差不多到现在已经有快六年的时间了,我的主要研究topic是包括large language model,以及language model里面的reasoning。我之前是在CMU拿的Machine Learning的PhD,我最喜欢的generative AI project,我其实对large language model所有的方面都比较感兴趣,我自己本身的topic是比较偏向于natural language reasoning这边的,所以我对如何unlock large language model的各种capability是非常感兴趣的。去年的话,其实有好多paper在这个方向都是很有意思的,我们自己组做的有包括train of thought和self consistency,都是用一些prompting的方法能够让large language model能够更好地做各种各样的reasoning,包括数学上的reasoning,或者common sense上,或者一些symbolic,就是模式性的reasoning,我觉得这些project都是很有意思的。在之前的话,我去年也有一些其他的paper,比如说有那种能让large language model做zero-shot reasoning的project,我觉得也是特别有意思的。大家好,我是Stability AI的技术产品总监,主要负责stable diffusion,推理,算法等等相关的一些工作。我的背景其实不是ML的科班出身,我之前是做产品的,在腾讯百度,然后后面去了咨询公司BCG,然后其实一直都做产品数字化等等的工作。2021年的时候,对,接触到了这些AI DC的技术,当时最早还是从VQGAN的paper开始,然后开始自己探索这些技术,后面就逐渐进入到这个领域,也在很多开源项目里面有一些贡献,那说,后面加入stability,我说到最喜欢的项目,其实就要回到2021年,当时有一个notebook,cliffguided diffusion,那可能是我最喜欢的一个项目,也是我相当于把我带入领域的这个notebook,是Katherine Carlson写的,它其实是整个,怎么说呢,最近很boom的stable diffusion等等的一个非常早期的,但是又是非常重要的一个工作,它验证了基于这个,比如我们现在做的inbeddings,比如基于一个小的预言模型,能够引导这一个diffusion model去生成它的整个图像的内容,然后也有很多社区的人开始加入,参与这个工作,参与这部分工作来探索这个应用。我好奇你是怎么加入到这个stability AI的,你可以简单介绍一下你这个report work的经历。好的,也是去年,去年年中的时候,当时我还在做我自己的consulting的项目,其实很忙啊,但是周六周日,因为我对这个技术特别感兴趣,周六周日我就花时间在写了自己的notebook,然后维护自己的开源项目上面,然后那个开源项目当年用的人不是很多,叫majority diffusion,但是它是在当年那个latent diffusion的那个时代,可以生成比较高质量的,包括人像啊,然后一些艺术品,就是AI艺术啊,这样的一个notebook,其实非常接近于stable diffusion,当然没有stable diffusion质量这么高啊。在这个建设这个开源项目的过程里面,就认识了很多社区的人,其中也包括stability的一些同事,然后当时在这个stability的那个时间点,有招揽一波开源开发者,我是在那一波被招揽的开源开发者里面加入了stability。嗯,我发现现在真的是这个全球,全球这个remote work的这个风潮,真的是,真的是让越来越多这个,借着这个开源社区的成长起来的公司,可以在全球范围内招揽到人才,接下来可能是一个非常有意思的一个趋势。好,谢谢一周这个这个分享,我发现果然呢,这次这次几位嘉宾都非常的硬核,真的可以就是跨过这个市场上的一些造诣,很多技术这个方面去了解到底技术的一些可为和不可为,我相信对大家应该也会很有帮助,当然我们会经常讲的深入浅出一些,那一文一文的可以来跟大家做一下自我介绍。好的,我叫一文,我现在在这个硅谷做这个ML的PM,然后我是应用物理出身,我本来的理想是做一个物理学家,那我主要做的,我研究的物理的这个课题主要是半导体物理,但是到了我毕业的时候实在是找不到工作,别人跟我说你可能挺适合做PM的,所以我就去,后来我说什么是PM,他们说你来了就知道了,我的职业生涯大概有五六年的时间一直在做硬件的PM,比如说我在苹果做了苹果手机,在亚马逊做了Amazon Echo,后来出去创业呢,第一次就是七八年前吧,第一次出去创业,接触了这个Community Vision,然后一直过去的七八年一直在做Community Vision,五年前我在Landing,我应该是Landing的第一批最早的一批员工了,Landing AI是文达老师创造的,然后他18年创办,主要是用AI来服务于制造业和农业,有一个方法就是,Stable Diffusion用的是Denoising的方法来生成图片,这个Denoising第一批使用Denoising来做Diffusion后处理的人,第一个提出这个东西的人叫Peter Beal,他其实是吴老师以前2019年还是2018年的博士生,再往早一点的话,最早是Gant,17年18年的时候我们也尽量大量的使用Gant来做生成式的模型,比较有名的就是像Psycho Gant,Beauty Gant,就像这种东西,其实Gant的提出人Ian Goodfellow他的硕士导师也是吴老师,说一说一个Fun Fact,就是一个有意思的项目,我有一个黑客朋友他做了一个App叫Draw Things,他应该是我知道的第一个把Stable Diffusion成功编译到手机上的应用,所以你现在如果下载Draw Things的话,你可以在iOS平台下载到它,然后你可以使用手机做Inference,实际上是很难的,因为哪怕是很好的台式机做Inference也是很费劲的,但是它是最早一批成功在手机上编译并且成功能够运行Inference的,如果大家有兴趣可以去下载一个。好呀好呀,果然这个黑客已经先行大一步了,这次也是邀请到了一位投资人朋友,Bill Xing,来跟我一起来做Co-host,要不Bill也可以跟大家简单介绍一下自己。Hello,大家好,我是五元资本的Bill,也是简单介绍一下自己,我之前在北大读的计算机,毕业之后就一直在做战略投资和财务投资,目前在五元的成长期的团队,主要是在看软件还有AI等底层技术创新的方向,说到关于AI的Fun Fact的话,我觉得比较有意思就是我今年从年初开始一直在关注Hacking Face这个平台,其实上面有非常多的开源的模型,然后新的AI相关的进展,开发者都会去Share在他的Space和Model Hub里面,因为之后也会谈到开源模型的商业模式。另外一块就是,其实我发现在推特上有非常多的AI相关的插件,比较有意思的还是说,有人用ChatGPT和搜索结合做了一个网页的插件版,这个我也是现在一直在使用,使得说现在这个ChatGPT的效果比原始版本对于新的一些新闻,然后对一些实时的内容的效果会更好了。自我介绍环节都非常有启发,那我觉得从一开始,因为的确前面提到ChatGPT 3.5的出现,的确让大家一下子破了圈,让感受到了AI的魔力,所以我想这个雪芝也是在Google做了大型语言模型,这一块的研究,我想我们所有关注ChatGPT的行业人,可能都会关注到Google也开发了一个大型的语言模型,这个Pathways,但对于一些可能还不那么熟悉的朋友,或者雪芝可以简单跟大家介绍一下,就是什么是Google POM,然后也可以简单的,可能high level的跟大家介绍一下,它在这种架构或者说一些你觉得比较重要的特点上,或者说像其他的像GPT 3,或者其他的一些这种语言模型有什么不一样的地方。好的,就Google的这个POM是在去年大概4月份左右发布的,然后POM是叫做Pathways Language Model,然后它是Google现在发布的,我想可能应该是市面上最大的一个Language Model,然后它最大的区别应该是它有540个边缘的parameter,然后它本身的架构是一个left to right,就是从左到右,然后decoder only,就只有一个decoder的transformer based language model,然后POM的优势在于首先它capacity特别大,因为它有540边缘的parameter,所以我们发布那个paper的时候,其实我们当时在paper里面也秀了,就是它在很多task上面都有非常好的performance,基本上都是state of the art,包括比如说正常的natural language processing,natural language understanding的task,或者是natural language generation的task,POM都是有很好,基本上是现在最好的performance,就是当时发布的时候最好的performance,然后另外我们在paper里面还写了,就是POM因为它大的capacity,它还顺带unlock了一些emergent abilities,比如说我们在里面举了一些例子,比如说你用一些prompt就可以让POM解释一些笑话,或者是可以让它做一些很复杂的task,比如说是reasoning task,然后这些能力是之前的一些小model所不能达到的,然后它跟GPT-3的具体区别,就GPT-3其实也是一个left to right,decoder only的language model,也是transformer based,然后可能最大的区别就是size上的difference,就是POM是一个540billion的parameter model,然后GPT-3是一个175billion parameter的language model,所以GPT-3要小很多,然后具体到一些training上detail上面的difference的话,那两个model是一样的,