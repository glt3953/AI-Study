欢迎来到Onboard,真实的一线经验,走新的投资思考。我是Monica。 我是高宁。我们一起聊聊软件如何改变世界。 大家好,欢迎来到Onboard,我是Monica。 自从OpenAI发布的ChatGPT掀起了席卷世界的AI热潮,不到三个月就积累了超过一亿的越活用户,超过1300万的日活用户。 真的是展现了AI让人惊叹的能力,也让很多人直呼这就是下一个互联网的未来。 有不少观众都说希望我们再做一期AI的讨论,于是这次硬核讨论就来了。 这次我们请来了Google Brain的研究员雪芝,她是Google大语言模型PALM Pathways Language Model的作者之一。 要知道,这个模型的参数量是GPT-3的三倍还多。 另外还有两位AI产品大牛,一位来自著名的Stable Diffusion背后的商业公司Certainty AI, 另一位来自某硅谷科技大厂,也曾在吴恩达教授的Landing AI中担任产品负责人。 此外,Monica还邀请到一位一直关注AI的投资人朋友Bill当作我的特邀共同主持嘉宾。 我们主要讨论几个话题,一方面从研究的视角,最前沿的研究者在关注什么? 现在的技术的天花板和未来大的变量可能会在哪里? 从产品和商业的角度,什么是一个好的AI产品? 整个生态可能随着技术有怎样的演变? 更重要的,我们又能从上一波AI的创业热潮中学到什么? 最后,Monica和Bill还会从投资人的视角做一个回顾、总结和畅想。 这里还有一个小的update,在本集发布的时候,Google也对爆发式增长的Chat GPT做出了回应。 正在测试一个基于Lambda模型的聊天机器人ApprenticeBot。 证实发布后会有怎样的惊喜?我们都拭目以待。 AI无疑是未来几年最令人兴奋的变量之一。 Monica也希望未来能邀请到更多一线从业者从不同角度讨论这个话题。 不论是想要做创业、研究、产品还是投资的同学, 希望这些对话对于大家了解这些技术演进、商业的可能,甚至未来对于我们每个人、每个社会意味着什么都能引发一些思考,提供一些启发。 这次的讨论有些技术硬核,需要各位对生成式AI大模型都有一些基础了解。 讨论中涉及到的论文和重要概念也会总结在本集的简介中,供大家复习参考。 几位嘉宾在北美工作生活多年,夹杂英文在所难免,也请大家体谅了。 欢迎来到未来,大家enjoy! 可以大家先做一个简单的自我介绍,你们自己过去的一些经验,一个fun fact,就是你最喜欢的一个生成式AI的项目。 大家好,我的名字叫王雪志,我现在是在Google Brain做research scientist。 我在Google差不多到现在已经有快6年的时间了,我的主要研究topic是包括large language model,以及language model里面的reasoning。 我之前是在join Google之前,我是在CMU拿的machine learning的PhD。 我最喜欢的generative AI project,我其实对large language model所有的方面都比较感兴趣, 我自己本身的topic是比较偏向于nature language reasoning这边的,所以我对如何unlock large language model的各种capability是非常感兴趣的。 去年的话,其实有好多papers在这个方向都是很有意思的,我们自己组做的有包括train of thought和self consistency, 都是用一些prompting的方法能够让large language model能够更好地做各种各样的reasoning,包括数学上的reasoning, common sense上或者一些symbolic,模式性的reasoning,我觉得这些project都是很有意思的。 在之前的话,我去年也有一些其他的papers,比如说有那种能让large language model做zero short reasoning的project,我觉得也是特别有意思的。 大家好,我是Stability AI的技术产品总监,主要负责stable diffusion,推理,算法等等相关的一些工作。 我的背景其实不是ML的科班出身,我之前是做产品的,在腾讯、百度,然后后面去了咨询公司BCG, 然后其实一直都做产品数字化等等的工作。 2021年的时候,对,接触到了这些AIDC的技术,当时最早还是从VQGAN的paper开始, 然后开始自己探索这些技术,后面就逐渐进入到这个领域,也在很多开源项目里面有一些贡献。 那说,后面加入stability,我说到最喜欢的项目,其实就要回到2021年,当时有一个notebook, cleave guided diffusion,那可能是我最喜欢的一个项目,也是我相当于把我带入领域的这个notebook,是Catherine Carlson写的。 她其实是整个怎么说呢,就是最近很boom的stable diffusion等等的一个非常早期的,但是又是非常重要的一个工作。 她验证了基于这个,比如我们现在做的这些embeddings,比如基于一个小的预言模型,能够引导这一个diffusion model去生成它的整个的图像的内容。 然后也有很多社区的人开始加入,参与这个工作,呃,参与这部分工作来探索这个应用。 我好奇你是怎么加入到这个stability AI的,你可以简单介绍一下你这个这个这个report work的经历。 好的,嗯,也是去年去年年中的时候,当时我还在做我自己的consulting的项目,其实很忙啊,但是周六周日,因为我对这个技术特别感兴趣,周六周日我就花时间在写了自己的notebook, 然后维护自己的开源项目上面,然后那个开源项目当年用的人不是很多,叫majestic diffusion,嗯,但是它是当在当年那个latent diffusion的那个时代,嗯,可以生成比较高质量的的这个。 包括人像啊,然后一些呃,艺术品,就是AI艺术啊,这样的一个notebook,其实非常接近于stable diffusion,当然没有stable diffusion质量这么高啊。 嗯,在在这个建设这个开源项目的过程里面就,嗯,认识了很多社区的人,包括其中也包括stability的一些同事,然后当时在这个stability的那个时间点有招揽一波开源开发者,我是在那一波被招揽的开源开发者里面加入的stability。 嗯,我发现现在真的是这个全球全球这个remote work的这个风潮真的是真的是让越来越多这个借着这个开源社区的成长起来的公司可以找在全球范围内招揽到人才,我说接下来可能是一个非常有意思的一个一个趋势。 好,谢谢一周这个这个分享,然后我发现果然呢,这次这次几位嘉宾都非常的硬核,真的可以就是跨过这个市场上的一些噪音,很多技术这个方面去了解到底技术的一些,呃,可为和不可为,我相信对大家应该也会很有帮助,当然我们会经常讲的深入浅出一些,呃,那呃,一文一般的可以来跟大家做一下自我介绍。 好的,呃,我叫一文,我现在在这个,呃,硅谷做这个ml的pm,然后我是应用物理出身,我本来是我本来的理想是做一个物理学家,那我主要做的我研究的物理的这个,呃,呃,课题主要是半导体物理,呃,但是到了我毕业的时候实在是找不到工作,呃,别人跟我说你可能挺适合做pm的,所以我就去后来我我说什么是pm,他们说你来了就知道了,我的职业生涯大概有。 五六年的时间一直在做硬件的pm,呃,比如说我在苹果做了苹果手机,在亚马逊做了m3口,呃,后来出去创业呢,呃,第一第一次就是七八年前吧,第一次出去创业,呃,接触了这个computer vision,然后一直过去的七八年一直在做computer vision。 嗯,几五年前我在landing,啊,我应该是landing的第一批最早的一批员工了,那来年是文达文达老师创造的,呃,然后,呃,他创他18年创办主要是用ai来呃,服务于制造业和农业,有一个方法就是呃,stable decision用的是denoising的方法来生成图片,这个denoising第一批使用denoising来生成图片。 来做这个做这个definition后处理的这个人,一个提出这个东西的人叫peter bill,他其实是吴老师,呃,以前2019年还是2018年的博士生,再往你再往早一点的话,呃,最早是干嘛,那17年18年的时候我们也尽量大量的使用干来做生成式的这个模型,那比较有名的就是像cycle干啊,beauty干啊,呃,就像这种东西,那其实干的提出人这个ean company,呃,他也有很多的,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃 good fellow他的硕士导师也是吴老师,那说一说一个fun fact就是一个有意思的项目,我有一个黑客朋友,他做了一个app叫做draw things,他应该是我知道的第一个把这个呃,save diffusion这个成功编译到手机上的呃,这么一个应用,所以你现在你如果现在下载那个draw things的话,你可以在这个呃,你可以在你可以在ls的呃,这个平台上下载到他,呃,然后你可以 你可以使用这个手机做influence,其他这次很难的,因为哪怕是台很好的台式机做influence也是很费劲的,但是他是最早一批呃,这个成功在手机上编译,并且成功能够运行influence的,如果大家有兴趣可以去下载一个 嗯,好呀好呀,果然这个黑客已经先行先行大一步,这个这次也是邀请到了呃,一位投资人朋友啊,bill行,来以后来跟我一起来做cohost,要不bill也可以跟大家简单介绍一下自己 哈喽哈喽大家好,我是五元资本的bill,也是简单介绍一下自己,我之前在北大读的计算机,然后毕业之后呢就一直在做战略投资和财务投资,那目前在五元的这个成长期的团队主要是在看软件还有ai等这个底层技术创新的这个方向,那说到关于ai的fan fact的话,我觉得比较有意思就是我今年从年初开始一直在关注hugging face这个平台,其实上面有非常多的开源的模型,然后呢,我一直在看这个平台的这个平台的这个平台的这个平台的这个平台的这个平台的这个平台的这个平台的这个平台的这个平台的这个平台的这个平台的这个平台的这个平台的 然后新的ai相关的这个进展开发者都会去share在他的这个space和monohub里面,因为之后也会谈到这个开源模型的商业模式,那另外一块就是 其实我发现在推特上有非常多的这个ai相关的这个插件,那比较有意思的还是说有人用这个 这个ChatGPT和搜索结合做了一个网页的插件版 那这个我也是现在一直在使用 使得说现在这个ChatGPT的效果比原始版本对于新的一些新闻 然后对一些实时的内容的效果会更好了 自我介绍环节都非常有启发 那我觉得从这个一开始 因为的确前面提到这个ChatGPT 3.5的出现 的确让大家一下子破了圈 让感受到了这个AI的这个魔力啊 所以我想这个雪芝也是在Google做了这个 呃这个大型语言模型这一块的这个研究 呃我想我们所有关很多关注这个ChatGPT的这个行业人 可能都会关注到这个Google也开发了一个 呃这个大型的语言模型这个pathways 但对于一些可能还不那么熟悉的朋友 呃或者雪芝可以呃简单跟大家这个介绍一下 就是什么是这个呃Google这个POM 然后也可以呃简单的这个可能high level的跟大家介绍一下 那它在这种呃架构啊或者说一些呃一些你觉得比较重要的特点上 跟其他的像呃GPT 3或者其他的一些这种呃语言模型有什么不一样的这个地方 哦好的 呃就Google的这个POM是在去年呃大概4月份左右发布的 然后POM是说呃是叫做pathways language model 然后它是Google现在发布的呃我想可能应该是市面上最大的一个language model 然后它是有它最大的区别应该是它有540个billion的parameter 然后它本身的架构是一个呃left to right就是从左到右 然后decoder only就只有一个decoder的那个transformer based language model 呃然后呃POM的优势在于呃首先它capacity特别大 因为它有540billion的parameter 所以呃我们发布那个paper的时候 其实我们当时在paper里面也show了就是它在很多task上面都有非常好的performance 基本上都是那个state of the art so 呃包括比如说正常的natural language processing 就natural language understanding的task 或者是natural language generation的task POM都是有很好就是这基本上是现在最好的performance 呃就是当时发布的时候最好的performance 然后另外我们在paper里面还写了就是POM因为它这个大的capacity 它还呃呃顺带unlock了一些emergent abilities 比如说呃我们在里面举了一些例子 比如说你用一些prompt就可以让POM解释一些笑话 或者是可以让它做一些很复杂的task 比如说是reasoning task 然后然后这些能力是之前的一些小model所不能达到的 然后呃它跟GPT-3的具体区别 就GPT-3其实也是一个left to right decoder only的language model 也是transformer based的 然后可能最大的区别就是size上的difference 就是POM是一个540billion的parameter的model 然后GPT-3是一个175billion的parameter的language model 所以GPT-3要小很多 然后具体到一些training上detail的上面的difference的话 呃那两个model的区别是什么呢