那两个model还是有一些细微的区别的 比如说POM的training corpus跟GPT-3是不一样的 然后另外一个比较重要的factor是说你在training的时候到底让这个model建过了多少token 就是number of tokens POM跟GPT-3也是不一样的 然后呃再具体到一些可能更细微的 我觉得可能很多人不是那么care的 呃细节可能是比如说 呃你POM里面做呃是怎么做tokenization的 然后GPT-3是怎么做tokenization 这可能这里面也有一些细微的区别 然后呃这些应该就是基本上最大的区别了吧 对 呃但是这两个东西就是basically还都是就是 呃你如果说下面架构的话都是一样的 都是一个 decoder only的transformer based language model 对 我有个问题就是问雪芝 就是那个我我看了一下那个GPT-3和GPT-2的论文哈 GPT-3的和GPT-2最大的问题最大的有个区别就是它它decoder后面的那些fine tuning的task在GPT-2里面都是zero short 所以它那个准确率特别差 然后在GPT-3里面它有些task就是one short还有few short learning的 然后一下就把那个就把那个呃准确率给干上去了 那这个呃这个方面的 嗯第一我第一个问题就是你觉得你觉得是不是从这个角度来说 呃数据在这个fine tuning task里面还是非常非常重要的 也就是说前面这decoder generalization就GPT-3这个generalization是呃能力还是有限的 这是第一个问题哈 然后第二个就是 PARM在这个decoder出去以后 它是怎么去处理那些呃特殊的这些呃各种不同的这个呃这个task呢 呃首先呃fine tuning跟few short是不矛盾的两件事情 不管是PARM也好GPT-2也好GPT-3也好 呃你在decoder就pre-train完了这个呃这个model之后 你都是可以either做fine tuning或者是做few short或者zero short 然后fine tuning的好处就是说 如果你有一定呃size的training data set的话 你就可以fine tune这个language model 然后让这个language modeladapt到你自己的这个specific task上面 然后它就可以在这个task上面performance特别好 然后few short跟zero short区别是说 假设你没有一个一定size的training set的话 比如说你现在有一个新的task 然后你完全没有任何data 然后你希望用这个language model去解决这个task 那你唯一可以做的就只有zero short跟few short 就你没有办法用fine tuning的方法 对 这是这是fine tuning跟zero shortfew short的具体区别 然后另外一个针对你第一个问题 其实是这样子 就是你其实做fine tuning或者zero shortfew short都是可以的 就在这三个model上都是可以做分别的事情的 取决于你有没有这个training set 但是GPT-2跟GPT-3最大的区别就是当GPT-3把capacity 就是它model capacity增长到175边缘的parameter的时候 它paper里面有写 就是它unlock了这个叫做in-context learning或者是few short的这个ability 就是说你model size在小的情况下 比如说GPT-2或者是在之前那些model 比如BERT或者T5 当model size很小的情况下 他们是不太拥有zero short或者few short的能力的 所以在这些model的情况下 你基本上只能用fine tuning的方法才能让model在某一个task上面达到一个可以的ok的performance 但是当GPT-3把model size增长到175边缘parameter的时候 他们就发现这个model自己就拥有了in-context learning或者这个叫做zero short或者few short的能力 然后在这种情况下 你不用再去further fine tune这个model 当然你也可以fine tune 但是instead of fine tuning 你其实可以直接对这个大size的language model做zero short或者few short 然后你会发现他的performance其实可以很好 然后这个能力在小size的model上面是不太存在的 就是你在GPT-2上面 你试这个zero short或者few short的performance就会特别的被差 这个是小model跟大model上面一个能力上的基本上的区别 我是不是可以这么理解 就是当你的前面的decoder足够大的时候 我很多的context 他所谓的in-context learning是不是因为前面的decoder足够大了以后 我的文本见过足够多 我能够学到的representation就足够多了 对 这个事情到现在为止还没有一个特别sorrow的分析 但是有很多paper试着去解释in-context learning的ability是怎么出来的 我们现在发现了有一个empirical finding 就是说到model size达到一定程度之后 然后这个一定程度其实也没有人发现它的cut off到底在什么地方 对 但是我们有一个empirical finding就是说如果model size达到一定程度 然后它的in-context learning的能力就自己出来了 这也是GPT-3那个paper里面它写的一个部分 它有一个subsection在介绍这个 对 然后你具体可以想 如果一定要解释为什么的话 其实有很多paper在解释这个为什么 有些paper把它解释成为一个类似于bias and inference的东西 就是首先是你model size足够大 然后你其实model里面承载了一定的数据量之后 然后这个model在做in-context learning的时候 其实就是说你给它几个类似的example 它就能从类似的example里面推导出你要做的这个task的pattern是什么 然后你现在给它一个这个task上面新的问题 它就能用它推导出来这个pattern去完成这个新的问题 就能完成这个task 就是像你说的实际上GPT-3它对这个context的这个理解能力的这个增长是 它到底rule cut是什么 其实有很多理论嘛 但是没有 我就没有看到特别简单的这个 没有看到特别简单的解释 而且在GPT-2出来了以后 GPT-3出来之前 就是如果我们看BERT和这个GPT-2 他们对那个language的semantics的理解 BERT是强于这个GPT-2的 但是到了GPT-3以后发现 突然发现它对这个context的理解 好像也不比BERT差多少 所以这就让 但是它其实不知道 它在每次 它是一个文本预测器嘛 你在预测后面的文本的时候 那你其实不知道后面的文本是什么 但是你居然能够 对这个context的理解比BERT这样知道了文本前文本后所有的信息的这样的分类器还要好 这个听起来就感觉 我就不是 其实我那个时候就觉得 这个完全超出我的理解的这个范畴 对 其实在这上面也有很多paper试图去解释 就是你到底encoder decoder好 还是decoder only好 然后这个东西也没有太 就是也没有定论 就是基本上的 就是其实encoder跟decoder没有什么太大区别 其实你想它们都是transformer based的 对吧 它们都是knowledge representation的一个方式 我觉得大部分paper其实最后 你就研究到最后就发现 你其实encoder出来跟decoder only出来 这个performance其实也没有差太多 对 然后你说BERT跟GPT-3比的话 那GPT-3还是比BERT 首先BERT就没有in-context learning的能力 对吧 GPT-3还是有in-context learning的能力的 然后其次你要fairly的compare这两个model 其实也不算很fair 因为你用BERT的话 你必须要进行funtuning对吧 然后GPT-3的话 它很多task它不用进行funtuning 它就用zero shot或者few shot的 其实就可以达到很好的performance 然后你拿zero shot的few shot的performance跟funtuning的performance比 其实也不是一个很fair的比较我感觉 对 这个我同意 然后另外一个是BERT很小 GPT-3很大 GPT-3有175米 BERT的各种size都是在million level的 所以这两个也不是可以比的 然后我觉得这里面可能最大的区别 如果一定要说GPT-3为什么比BERT在很多task上面能力要强很多的话 我觉得是因为GPT-3 你想如果它 因为首先它capacity更大 它自然有175编量的parameter 然后其次它training的时候见过的token也会更多 如果你想如果一个小孩子学东西的话 它如果比如说它只读过像BERT是只读过wikipedia跟book corpus的 它有两个唯一的pre-training corpus 然后GPT-3是读过web上的很多更多就是更高coverage的corpus 你想如果让同一个小孩去读这两个corpus的话 那那个小孩最后学的就哪个能力会更强 对吧 那肯定是读过更多corpus的那个小孩更强 应该是可以这么解释 我们看到就是我们看到包括这个微软啊什么各个公司也都在推出自己的大模型 那我们未来会看到就是这些模型都在往越来越多的参数去走嘛 在这个增加参数过程中 我会看到哪一些可能的这个挑战和瓶颈 或者说还有哪一些除了增加参数之外 就是对于提高这个模型这个能力还能够有比较大的影响的一些改进呢 从我这边来讲的话 我觉得可能另外一个最重要的factor 其实是去年一篇paper 那个paper是从deepmind来的 然后那paper叫Chichina 然后他那个title好像是什么training compute optimal language models 对 然后他里面最重要的一个finding是说in addition to model size 可能最大的另外一个区别就是你在training过程中建过的token数 对 然后他们发现比如说你把同样一个language model 你比如说把它pre-train在一个地方 他们提了一个Chichina那个model 然后你那个地方可能并不是optimal的 你有时候可能pre-train 你比如说定了一个model size175b 然后你有一个training corpus 然后你train到某一个point你就停了 对吧 他们发现其实那个point可能并不是最优的point 你如果把那个同样的model 你什么事情都不变 你让他继续train 就让他建过的token更多的话 这个model的capability还是会继续增长的 然后他在那个paper里面推了一个就是optimal的公式 就是说你对于每个model size的话 model capacity的话 你train 你让他建多少个token是最优的 对 然后这个事情其实非常重要 也就是说除了model size之外 你这个model在pre-training过程中 到底建过多少个token 也是一个非常非常重要的factor 就是决定了最后model的capacity model performance会怎么样 然后像我们这个palm paper其实也是有类似 我们其实用那个推论也去证明了一下 做palm我们4月份发那个paper的时候 所有的model 包括我们62币的model 540币的model 建过的token size是一模一样的 都是接近于0.8trillion的token 应该是0.78到0.79trillion的token之间 然后我们去年大概8月份9月份的时候 用Churchill的idea去把palmcontinue pre-training 就是让他建过的token sizedouble了 就是我们后来推出了一个 你可以看我们现在palm paper有一个appendix 然后那个appendix有一个model叫做palm trailer palm trailer就是说我们把palm 62币的那个model 他原来就第一次pre-training的时候建过的token数大概是0.8t 然后我们把它double了 就是我们continue pre-training了 一模一样的时间 就是让他建过的tokendouble了 然后我们发现这个62币的这个palm model 其实很多task上performance甚至超过了540币的那个model的performance 所以等于说你model size固然是一个重要的factor 但其实还有一个很重要的factor就是这个model在pre-training过程中 到底建过了多少token 就是他建过的token数 也就是其实你可以想象 如果是比喻成一个小孩他看书的话 他你可能有个web conference对吧 那他是看了一半的web conference 还是看了就所有的web conference 他web conference看了一遍 还是看了两遍 其实对他最后的performance也是有很大影响的 就是分了两三年 我们会看到就是越来越大的这个large language model 还是说你们现在感觉其实他这个这个参数的这个增长 其实已经到了一个平静期 呃 这个事情首先我觉得scale up是一个easy route 就很多人还是会take这个route 因为你永远不知道他的limit在哪对吧 然后scale up就是一个就是相当于你是一个很容易往上走的方法 你就是throw in更多computation power就可以的事情 所以很多人还是会继续scale up的 我不觉得现在model到了limit 然后其实不是有很多呃有很多新闻说比如说GPT4 有可能是一个trillion size的model嘛对吧 所以scale wise我觉得还是有继续往上升的空间的 然后其次呃从那个Chichina或者PalmTree的performance的结果 我们可以看到就是呃在model size之外 你见过的token数就是我们我们这个compass其实到现在还是没有饱和的 就是我觉得你呃对你你model size可以继续往上scale up 当然如果你的compass变得更大的话 你model size自然也得scale up 不然的话这个model也记不住那么多的东西对吧 所以呃我觉得往上scale这个还是应该是有一定空间的 然后这个limit在哪现在也没有人知道 因为还没有人scale到那个地步 不过也有可能很快就有人知道 对呃然后另外一个就是我觉得呃现在很多study都说 其实我们现在即使我们有很大的web compass 但这个web compass我们现在train的时候还是sample过的 所以它里面还是有很多tail knowledge 可能language model是记得不是很好的 所以这里面还是有空间就是让model见到更多的呃这个compass里面的内容 然后更好的记住里面的tail knowledge这些东西 我想把research和产品化稍微分开一眼 从research和模型建立的角度上来说完全同意scale up是一个easy road 而且它它可见的就会有比较好的效果 但从deployment上面来说 其实即便是我们现在说的palm或者说GP3 GP4马上要来的GP4这些它100多B的参数量 其实对于推理来说已经会有一个比较高的成本了 然后更不要说我们还要再去scale up 就即便是我们说硬件在进步 我们的架构在进步 但几几几百B的这样的参数量对于计算来说都会是一个很大的一个burden