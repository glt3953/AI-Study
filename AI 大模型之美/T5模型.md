# T5
来自 Google 的 T5 模型的全称是 Text-to-Text Transfer Transformer，翻译成中文就是“文本到文本的迁移 Transformer”，T5 最大的模型也有 110 亿个参数，也是基于 Transformer。是适合做迁移学习的一个模型，所谓迁移学习，也就是它推理出来向量的结果，常常被拿来再进行机器学习，去解决其他自然语言处理问题。T5模型的特点是可以通过对输入文本进行不同任务的预处理，来实现多种自然语言处理任务，如文本分类、机器翻译、问答等。T5模型在多个自然语言处理任务上取得了最先进的结果，成为当前最具代表性的预训练语言模型之一。
## 类型
1. T5-Base：基础版，包含11亿个参数，适合一般的NLP任务。
2. T5-Small：小型版，包含6亿个参数，适合轻量级NLP任务。
3. T5-Large：大型版，包含77亿个参数，适合复杂的NLP任务。
4. T5-3B：3亿个参数的T5模型，用于处理大规模数据集的NLP任务。
5. T5-11B：11亿个参数的T5模型，是目前最大的T5模型，适用于处理超大规模的NLP任务。
1. T5 Small：具有60M参数
2. T5 Base：具有220M参数
3. T5 Large：具有770M参数
4. T5 3B：具有3B参数
5. T5 11B：具有11B参数

这些模型的大小随着参数数量的增加而增加，可以应用于不同的任务和应用程序中。
## T5ForConditionalGeneration
T5ForConditionalGeneration.from_pretrained('t5-base').generate() 是T5模型的生成接口，可以根据给定的输入生成相应的输出。该接口的具体使用方法如下：

1. 导入T5ForConditionalGeneration模型和tokenizer

```python
from transformers import T5ForConditionalGeneration, T5Tokenizer

model = T5ForConditionalGeneration.from_pretrained('t5-base')
tokenizer = T5Tokenizer.from_pretrained('t5-base')
```

2. 给定输入，使用tokenizer对输入进行编码

```python
input_str = "translate English to French: Hello, how are you?"
input_ids = tokenizer.encode(input_str, return_tensors='pt')
```

3. 使用model.generate()方法生成输出

```python
outputs = model.generate(input_ids)
```

4. 使用tokenizer对输出进行解码

```python
output_str = tokenizer.decode(outputs[0], skip_special_tokens=True)
```

完整的代码示例：

```python
from transformers import T5ForConditionalGeneration, T5Tokenizer

model = T5ForConditionalGeneration.from_pretrained('t5-base')
tokenizer = T5Tokenizer.from_pretrained('t5-base')

input_str = "translate English to French: Hello, how are you?"
input_ids = tokenizer.encode(input_str, return_tensors='pt')
outputs = model.generate(input_ids)
output_str = tokenizer.decode(outputs[0], skip_special_tokens=True)

print(output_str)
# Output: Bonjour, comment vas-tu?
```
## 删除模型
在Mac系统中搜索已下载的T5模型，可以按以下步骤操作：
1. 打开 Finder，点击菜单栏中的“前往”，选择“前往文件夹”。
2. 在弹出的对话框中输入文件路径，一般T5模型默认下载到~/.cache/huggingface/hub/目录下。
3. 点击“前往”按钮，即可进入该目录。
4. 在该目录下可以找到已下载的T5模型。
